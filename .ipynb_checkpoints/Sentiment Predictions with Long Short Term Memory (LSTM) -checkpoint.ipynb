{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB movie review dataset\n",
    "Keras provides a **one-dimensional convolutional net** to examine the **IMDB movie review dataset**. Each data point is prelabeled with a `0` (**negative sentiment**) or a `1` (**positive sentiment**). However, we are going to swap out their example IMDB movie review dataset for one in raw text, so we can get our hands dirty with the preprocessing of the text as well. We’ll use the trained model to classify novel review text it has never seen before.\n",
    "\n",
    "This raw dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification.\n",
    "The core dataset contains `50,000` reviews split evenly into `25,000` train and `25,000` test sets. The overall distribution of labels is balanced (`25,000` pos and `25,000` neg). In addition to the review text files, the maintainers of the dataset include already-tokenized bag of words (BoW) features that were used in their experiments (we are not going to use the BOW but prepare ours from the raw dataset). *See the dataset maintainers' README file contained in the release for more details*.\n",
    "\n",
    "Link to dataset: https://ai.stanford.edu/%7eamaas/data/sentiment/ **Learning Word Vectors for Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data\n",
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", str(text))\n",
    "    return re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filepath):\n",
    "    \"\"\"This function shall be used to load and preprocess both train and test datasets\"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            dataset.append((pos_label, clean_reviews(f.read())))\n",
    "            \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            dataset.append((neg_label, clean_reviews(f.read())))\n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the *target labels* from the loaded datasets...\n",
    "### Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(dataset):\n",
    "    \"\"\" Extract the target labels from the dataset \"\"\"\n",
    "    target_labels = []\n",
    "    for sample in dataset:\n",
    "        target_labels.append(sample[0])\n",
    "    return target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenizer and Vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our *feature engineering*, we are going to employ Google's **Word2vec** model developed by *Thomas Mikolov and team* in 2013 to generate Word2Vec embeddings. The word vector representation from Word2vec **captures much more specific and more precise meaning or semantics of the target word** than the  word-topic vectors generated by **Latent Semantic Analysis (LSA)** and **Latent Dirichlet allocation (LDiA)**.\n",
    "\n",
    "We are limiting our vocab to just `500,000` words due to lack of sufficient memory. This means our *Google Word2vec* word vectors would not contain all the words in our dataset. For such no match cases we shall skip those words during tokenization in order to bypass the errors and continue with the rest of the words.\n",
    "Google's Word2vec binary file source: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file = '../word2vec-GoogleNews-vectors/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_file, binary = True, limit = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass                                   # if no matching token in the Google w2v vocab\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the thought vector size\n",
    "The size of our thought vector in very important because it determines the number of time steps and the number of weights in the feed forward layer to train. But most importantly the size of our thought vector determines the *distance* the **backpropagation had to travel** each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_len(data, maxlen):\n",
    "    total_len = truncated = exact = padded = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample)\n",
    "        if len(sample) > maxlen:\n",
    "            truncated += 1\n",
    "        elif len(sample) < maxlen:\n",
    "            padded += 1\n",
    "        else:\n",
    "            exact += 1\n",
    "    print('Padded: {}'.format(padded))\n",
    "    print('Equal: {}'.format(exact))\n",
    "    print('Truncated: {}'.format(truncated))\n",
    "    print('Avg length: {}'.format(total_len/len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pre_process_data(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I bought the video rather late in my collecting and probably would have saved a lot of money if I bought it earlier  It invariably supersedes anything else on those  Cosmo s moon  nights  Cher and Olympia certainly deserve their awards but this is really a flawless ensemble performance of a superb screenplay  What  You don t know what a  Cosmo s moon  is ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train_data = tokenize_and_vectorize(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_labels = collect_labels(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../aclImdb/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pre_process_data(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'The Nest is really just another  nature run amock  horror flick that fails because of the low budget  The acting is OK  and the setting is great  but somehow the whole film just seemed a bit dull to me  The gore effects are not the best I ve seen but are fun in a cheesy sort of way  The roaches themselves are just regular cockroaches that bite people  The Nest reminded me of a much better film called Slugs  If you liked The Nest then Slugs is a must see as it s ten times better  Also worth noting is that Lisa Langlois who plays Elizabeth was in another  nature run amock  type film called Deadly Eyes  aka The Rats   which is about killer rats as you may have guessed   br    br   If you enjoy these types of horror films then you may want to give this a watch  but you d be far better off seeing Slugs which is far more interesting and gory ')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have `25,000` training samples and `25,000` test samples as expected, void of most of the html characters and special symbols. The next step is to *tokenize* and *vectorize* the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test_data = tokenize_and_vectorize(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_labels = collect_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall pad/truncate our train and test data, convert it to *numpy arrays* as required by Keras for its optimized vectorized operations. This is a *tensor* with the shape (**number of samples**, **sequence length**, **word vector length**) that we need for our GRU model. **We won’t usually need to pad or truncate with RNNs (LSTMs, GRUs), because they can handle input sequences of variable length**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded: 22150\n",
      "Equal: 17\n",
      "Truncated: 2833\n",
      "Avg length: 215.54484\n"
     ]
    }
   ],
   "source": [
    "test_len(vectorized_train_data, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "num_neurons = 50\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to pad tokens\n",
    "Keras has a preprocessing helper method, **pad_sequences**, that in theory could be used to pad our input data, but unfortunately *it works only with sequences of scalars*, but we have *sequences of vectors*. Let’s write a helper function to pad our input sequence of vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\"\n",
    "    For a given dataset pad with zero vectors or truncate to maxlen\n",
    "    \"\"\"\n",
    "    # This one-liner can accomplish the same task!\n",
    "    # return [sample[:maxlen] + [[0.] * embedding_dims] * (maxlen - len(sample)) for sample in data]\n",
    "    \n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_trunc(vectorized_train_data, maxlen)\n",
    "X_test = pad_trunc(vectorized_test_data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
    "y_train = np.array(target_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\n",
    "y_test = np.array(target_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07910156, -0.0050354 ,  0.11181641, ..., -0.0067749 ,\n",
       "         0.04272461, -0.10351562],\n",
       "       [ 0.16699219, -0.05419922, -0.08740234, ..., -0.125     ,\n",
       "         0.18457031, -0.21484375],\n",
       "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
       "         0.04760742, -0.06884766],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08056641, -0.0390625 , -0.140625  , ..., -0.01385498,\n",
       "         0.00958252, -0.2421875 ],\n",
       "       [ 0.17285156,  0.40625   , -0.3046875 , ..., -0.31445312,\n",
       "         0.08984375,  0.26953125],\n",
       "       [-0.05859375, -0.03759766,  0.07275391, ..., -0.10791016,\n",
       "        -0.08642578, -0.03198242],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM model\n",
    "### Initialize model and add a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(\n",
    "    num_neurons, \n",
    "    return_sequences = True,\n",
    "    input_shape = (maxlen, embedding_dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our sequences are `200` (maxlen) tokens long and we’re using `50` hidden **neurons**, our output from this layer will be a vector of `200` elements long. Each of those elements is a vector `50` elements long, with **one output for each of the neurons** (Each token is processed by `50` neurons to out a vector of length `50` bundled into an output vector of length `200`). The output is therefore a list of lists where the inner list are `200` and each is of length `50`!\n",
    "The keyword argument `return_sequences` will tell the network to return the network value at each *time step*, hence the `200` vectors, each `50` elements long. If `return_sequences` is set to *False*, the model will be a feedforward and *not* an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a dropout and output layers\n",
    "To prevent *overfitting* we add a **Dropout layer** to zero out `20%` of those inputs, randomly chosen on each input example. And then finally we add a classifier. In this case, we have **binary classification** task: *Yes or Positive Sentiment* is labeled `1` and *No or Negative Sentiment* is labeled `0`. So we chose a layer with one neuron (Dense(1)) and a `sigmoid` activation function. But a Dense layer expects a “*flat*” vector of **n elements** (each element a float) as input. And the data coming out of the *GRU* is a *tensor* `200` elements long, and each of those are `50` elements long. So we use a `Flatten()` layer to flatten the input from a `200 x 50` tensor to a vector `10,000` elements long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 200, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 80,201\n",
      "Trainable params: 80,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile('rmsprop', 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 164s 8ms/sample - loss: 0.4564 - acc: 0.7891 - val_loss: 0.3648 - val_acc: 0.8496\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 116s 6ms/sample - loss: 0.3441 - acc: 0.8544 - val_loss: 0.3243 - val_acc: 0.8650\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 42s 2ms/sample - loss: 0.2866 - acc: 0.8856\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 88.56%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: {}%'.format(round(float(train_accuracy) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 85.73%\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy: {}%'.format(round(sum(val_acc) / len(val_acc) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 90s 4ms/sample - loss: 0.3268 - acc: 0.8596\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.96%\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: {}%'.format(round(float(test_accuracy) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have comparable validation and test accuracies of `85.73%` and `85.96%` respectively. This an indication that *overfitting* of the training data is not a huge problem. However, with a training accuracy of more than `88.56%` and a test accuracy of `85.96%` the model can still be tweaked. \n",
    "### Save the model\n",
    "Saving both the model architecture and its weights will allow it to be reloaded and trained from that point on if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"./model/lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"./model/lstm_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/lstm_model.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.model_from_json(json_string)\n",
    "loaded_model.load_weights('./model/lstm_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test example\n",
    "Get some sample reviews from the internet and use the model to predict the reviewers' sentiments regarding the movie...(make sure to remove any sensitive issues like names and replace them with make up ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\"Loved the film. I wasn’t sure at the start but it was lovely \"+ \\\n",
    "               \"to see all the characters from the small screen arrive in the \"+ \\\n",
    "               \"cinema as old friends, and I laughed and cried. This is a great \"+ \\\n",
    "               \"film and I really hope they make a sequel. \"+ \\\n",
    "               \"To the person who gave this film one star you should have reviewed \"+ \\\n",
    "               \"the film, not the taxi driver and as you didn’t see the first 30 minutes \"+ \\\n",
    "               \"you aren’t in a position to comment on the entire film anyway.\", \n",
    "               \n",
    "               \"I have no doubt that Uptown fans will support this film. We have every \"+ \\\n",
    "               \"episode on DVD, so it is with something of a heavy heart to give this film \"+ \\\n",
    "               \"such a low rating. As a stand-alone film (or if you have never seen the TV series), \"+ \\\n",
    "               \"all you get are lavish scenery and costumes. However, the characters appear shallow \"+ \\\n",
    "               \"and the plot flimsy. As a Uptown fan, yes – the pleasant and familiar characters are \"+ \\\n",
    "               \"there for you to enjoy in their familiar costumes. However, that is not enough. \"+ \\\n",
    "               \"Soon into the film, we found that the depth of our characters was not there. \"+ \\\n",
    "               \"I can only allude to metaphors. It was like watching a Formula One race run at \"+ \\\n",
    "               \"20 mph – where was the excitement? It was like watching a Weakenhand ruby game of \"+ \\\n",
    "               \"touch rugby – all spectacle but no impact. It was like being forced to lie in a bubble \"+ \\\n",
    "               \"bath of lukewarm water for too long. Even a couple of people around gave up and stated \"+ \\\n",
    "              \"playing with their iPhone, with mutterings as we left at it was far too long. \"+ \\\n",
    "               \"Maybe it was our local cinema’s projection but even the film quality was nothing \"+ \\\n",
    "               \"like my Blu-ray at home, let along 4K. So, all in all, this is best seen as a light touch \"+ \\\n",
    "               \"homage to the TV series. Bearing in mind the trouble taken to assemble the actors in one \"+ \\\n",
    "               \"place at one time to make this movie, this was a wasted opportunity to create a real Uptown epic. \"+ \\\n",
    "               \"I hope they do not make a sequel.\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = clean_reviews(review_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sample_1 = tokenize_and_vectorize([(1, sample_1)])                                            # tokenize and vectorize\n",
    "test_vec_sample_1 = pad_trunc(vec_sample_1, maxlen)                                               # padding / truncate\n",
    "test_vec1 = np.reshape(test_vec_sample_1, (len(test_vec_sample_1), maxlen, embedding_dims))       # reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class: [[1]]\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment class: {}'.format(loaded_model.predict_classes(test_vec1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2 = clean_reviews(review_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sample_2 = tokenize_and_vectorize([(1, sample_2)])                                           \n",
    "test_vec_sample_2 = pad_trunc(vec_sample_2, maxlen)                                               \n",
    "test_vec2 = np.reshape(test_vec_sample_2, (len(test_vec_sample_2), maxlen, embedding_dims))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class: [[1]]\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment class: {}'.format(loaded_model.predict_classes(test_vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_3 = 'The story has no center; the duck is not likable, and the costly, overwrought, laser-filled special effects '+ \\\n",
    "'that conclude the movie are less impressive than a sparkler on a birthday cake. James ‘Star Wars’ Luke supervised the ' + \\\n",
    "'production of this film, and maybe it’s time he went back to making low-budget films like his best picture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_3 = clean_reviews(sample_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sample_3 = tokenize_and_vectorize([(1, sample_3)])                                           \n",
    "test_vec_sample_3 = pad_trunc(vec_sample_3, maxlen)                                               \n",
    "test_vec3 = np.reshape(test_vec_sample_3, (len(test_vec_sample_3), maxlen, embedding_dims))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class: [[0]]\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment class: {}'.format(loaded_model.predict_classes(test_vec3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to be doing a good job!\n",
    "\n",
    "The `predict_classes()` method gives the expected `0` or `1` for a binary classification task. The `.predict()` method reveals the raw `sigmoid` activation function output (a continuous value between `0` and `1`) before thresholding. Anything **above** `0.5` will be classified as positive (`1`) and **below** `0.5` will be negative (`0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output of sigmoid function for sample_1: [[0.8180467]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw output of sigmoid function for sample_1: {}\".format(loaded_model.predict(test_vec1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output of sigmoid function for sample_2: [[0.63209873]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw output of sigmoid function for sample_2: {}\".format(loaded_model.predict(test_vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output of sigmoid function for sample_3: [[0.36620152]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw output of sigmoid function for sample_3: {}\".format(loaded_model.predict(test_vec3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
